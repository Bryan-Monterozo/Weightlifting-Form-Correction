{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryan-Monterozo/Weightlifting-Form-Correction/blob/main/Test_Form_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2-4JFOqPSDA"
      },
      "source": [
        "# **INIT WORKSPACE** (Clone Github)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeghN5CZPcaB"
      },
      "source": [
        "## Clone Repo (https://github.com/NgoQuocBao1010/Exercise-Correction.git)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBfJIJLGPpdD"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NgoQuocBao1010/Exercise-Correction.git\n",
        "%cd Exercise-Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzMcnKbXPyd0"
      },
      "source": [
        "## Initialize Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqCzaRl8P6lH"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcTU4-PFPAWt"
      },
      "source": [
        "# **TEST BICEPS CURLS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sa5rva8PHD2"
      },
      "source": [
        "## Dataset Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C63SNk_7RV7c"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Run debug below if mediapipe module didn't install properly"
      ],
      "metadata": {
        "id": "MocDUrpFKvP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYApRd5PRrL2"
      },
      "outputs": [],
      "source": [
        "# DEBUG\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv_jupyter_ui\n",
        "!pip install -q ipycanvas==0.11\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogfn1wb3t2_z",
        "outputId": "dfc97388-763a-4eeb-d82b-f46abfc08c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/255.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/255.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAB9KxJbPNY2"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import opencv_jupyter_ui as jcv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, csv\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Drawing helpers\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "jcv2.setKeys(['q','c','l'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zElAl81XSEPM"
      },
      "source": [
        "### 1. Describe the data gathering process and build dataset from Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kpWPsuSSnc"
      },
      "source": [
        "C = Correct <br>\n",
        "L = Lean Back Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qHmz3r9SafM"
      },
      "outputs": [],
      "source": [
        "# Determine important landmarks for plank\n",
        "IMPORTANT_LMS = [\n",
        "    \"NOSE\",\n",
        "    \"LEFT_SHOULDER\",\n",
        "    \"RIGHT_SHOULDER\",\n",
        "    \"RIGHT_ELBOW\",\n",
        "    \"LEFT_ELBOW\",\n",
        "    \"RIGHT_WRIST\",\n",
        "    \"LEFT_WRIST\",\n",
        "    \"LEFT_HIP\",\n",
        "    \"RIGHT_HIP\",\n",
        "]\n",
        "\n",
        "# Generate all columns of the data frame\n",
        "\n",
        "HEADERS = [\"label\"] # Label column\n",
        "\n",
        "for lm in IMPORTANT_LMS:\n",
        "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfEwvMjSiVe"
      },
      "source": [
        "#### 1.2 Set up important functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXa-FjTESr4g"
      },
      "outputs": [],
      "source": [
        "def rescale_frame(frame, percent=50):\n",
        "    '''\n",
        "    Rescale a frame to a certain percentage compare to its original frame\n",
        "    '''\n",
        "    width = int(frame.shape[1] * percent/ 100)\n",
        "    height = int(frame.shape[0] * percent/ 100)\n",
        "    dim = (width, height)\n",
        "    return cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "def init_csv(dataset_path: str):\n",
        "    '''\n",
        "    Create a blank csv file with just columns\n",
        "    '''\n",
        "\n",
        "    # Ignore if file is already exist\n",
        "    if os.path.exists(dataset_path):\n",
        "        return\n",
        "\n",
        "    # Write all the columns to a empty file\n",
        "    with open(dataset_path, mode=\"w\", newline=\"\") as f:\n",
        "        csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        csv_writer.writerow(HEADERS)\n",
        "\n",
        "\n",
        "def export_landmark_to_csv(dataset_path: str, results, action: str) -> None:\n",
        "    '''\n",
        "    Export Labeled Data from detected landmark to csv\n",
        "    '''\n",
        "    landmarks = results.pose_landmarks.landmark\n",
        "    keypoints = []\n",
        "\n",
        "    try:\n",
        "        # Extract coordinate of important landmarks\n",
        "        for lm in IMPORTANT_LMS:\n",
        "            keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
        "            keypoints.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
        "\n",
        "        keypoints = list(np.array(keypoints).flatten())\n",
        "\n",
        "        # Insert action as the label (first column)\n",
        "        keypoints.insert(0, action)\n",
        "\n",
        "        # Append new row to .csv file\n",
        "        with open(dataset_path, mode=\"a\", newline=\"\") as f:\n",
        "            csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "            csv_writer.writerow(keypoints)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "\n",
        "def describe_dataset(dataset_path: str):\n",
        "    '''\n",
        "    Describe dataset\n",
        "    '''\n",
        "\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    print(f\"Headers: {list(data.columns.values)}\")\n",
        "    print(f'Number of rows: {data.shape[0]} \\nNumber of columns: {data.shape[1]}\\n')\n",
        "    print(f\"Labels: \\n{data['label'].value_counts()}\\n\")\n",
        "    print(f\"Missing values: {data.isnull().values.any()}\\n\")\n",
        "\n",
        "    duplicate = data[data.duplicated()]\n",
        "    print(f\"Duplicate Rows : {len(duplicate.sum(axis=1))}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def remove_duplicate_rows(dataset_path: str):\n",
        "    '''\n",
        "    Remove duplicated data from the dataset then save it to another files\n",
        "    '''\n",
        "\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df.drop_duplicates(keep=\"first\", inplace=True)\n",
        "    df.to_csv(f\"cleaned_train.csv\", sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "def concat_csv_files_with_same_headers(file_paths: list, saved_path: str):\n",
        "    '''\n",
        "    Concat different csv files\n",
        "    '''\n",
        "    all_df = []\n",
        "    for path in file_paths:\n",
        "        df = pd.read_csv(path, index_col=None, header=0)\n",
        "        all_df.append(df)\n",
        "\n",
        "    results = pd.concat(all_df, axis=0, ignore_index=True)\n",
        "    results.to_csv(saved_path, sep=',', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7pdjBhBSzXJ"
      },
      "source": [
        "### 2. Extract data from video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANtPwxpQS3YS"
      },
      "source": [
        "NOTE: Re-check Directory and Filename <br>\n",
        "debug code for keypress simulation (<font color='red'>colab not working with key presses</font>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GmwvRb6TAGa"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"train.csv\"\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/Exercise-Correction/demo/bc_demo.mp4\")\n",
        "save_counts = 0\n",
        "\n",
        "# init_csv(DATASET_PATH)\n",
        "\n",
        "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Reduce size of a frame\n",
        "        image = rescale_frame(image, 60)\n",
        "        image = cv2.flip(image, 1)\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        results = pose.process(image)\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "            print(\"Cannot detect pose - No human found\")\n",
        "            continue\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image.flags.writeable = True\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=4), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
        "\n",
        "        # Display the saved count\n",
        "        cv2.putText(image, f\"Saved: {save_counts}\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        jcv2.imshow('train', image)\n",
        "\n",
        "        # Pressed key for action\n",
        "        k = jcv2.waitKey(1) & 0xFF\n",
        "\n",
        "        # Press C to save as correct form\n",
        "        if k == ord('c'):\n",
        "            export_landmark_to_csv(DATASET_PATH, results, \"C\")\n",
        "            save_counts += 1\n",
        "        # Press L to save as low back\n",
        "        elif k == ord(\"l\"):\n",
        "            export_landmark_to_csv(DATASET_PATH, results, \"L\")\n",
        "            save_counts += 1\n",
        "\n",
        "        # Press q to stop\n",
        "        elif k == ord(\"q\"):\n",
        "            break\n",
        "        else: continue\n",
        "\n",
        "    cap.release()\n",
        "    jcv2.destroyAllWindows()\n",
        "\n",
        "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
        "    #for i in range (1, 5):\n",
        "     #   cv2.waitKey(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Clean and Visualize Data"
      ],
      "metadata": {
        "id": "kW5aAn9JE05s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_duplicate_rows(\"./train.csv\")"
      ],
      "metadata": {
        "id": "LOjPXmSXF1wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = describe_dataset(\"./train.csv\")\n",
        "sns.countplot(x='label', data=df, palette=\"Set1\")"
      ],
      "metadata": {
        "id": "1dPz2U-9GCXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Gather Test Dataset"
      ],
      "metadata": {
        "id": "Qm-VHj3OGMTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_DATASET_PATH = \"test.csv\"\n",
        "\n",
        "cap = cv2.VideoCapture(\"../data/db_curl/bc_test_2.mp4\")\n",
        "save_counts = 0\n",
        "\n",
        "init_csv(TEST_DATASET_PATH)\n",
        "\n",
        "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Reduce size of a frame\n",
        "        image = rescale_frame(image, 60)\n",
        "        image = cv2.flip(image, 1)\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        results = pose.process(image)\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "            print(\"Cannot detect pose - No human found\")\n",
        "            continue\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image.flags.writeable = True\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=4), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
        "\n",
        "        # Display the saved count\n",
        "        cv2.putText(image, f\"Saved: {save_counts}\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        jcv2.imshow(\"test\", image)\n",
        "\n",
        "        # Pressed key for action\n",
        "        k = jcv2.waitKey(10) & 0xFF\n",
        "\n",
        "        # Press C to save as correct form\n",
        "        if k == ord('c'):\n",
        "            export_landmark_to_csv(TEST_DATASET_PATH, results, \"C\")\n",
        "            save_counts += 1\n",
        "        # Press L to save as low back\n",
        "        elif k == ord(\"l\"):\n",
        "            export_landmark_to_csv(TEST_DATASET_PATH, results, \"L\")\n",
        "            save_counts += 1\n",
        "\n",
        "        # Press q to stop\n",
        "        elif k == ord(\"q\"):\n",
        "            break\n",
        "        else: continue\n",
        "\n",
        "    cap.release()\n",
        "    jcv2.destroyAllWindows()\n",
        "\n",
        "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
        "    # for i in range (1, 5):\n",
        "    #   cv2.waitKey(1)"
      ],
      "metadata": {
        "id": "IpsuwoC4HUw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = describe_dataset(TEST_DATASET_PATH)\n",
        "sns.countplot(y='label', data=test_df, palette=\"Set1\")"
      ],
      "metadata": {
        "id": "g1LmeZejIPXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model Using SKLearn"
      ],
      "metadata": {
        "id": "yI_Dw3K4JGQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "wuhamhXZKf53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Run debug below if mediapipe module didn't install properly"
      ],
      "metadata": {
        "id": "eu88CgI5L7D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEBUG\n",
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "Ib2Uns_cLuY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv_jupyter_ui\n",
        "!pip install -q ipycanvas==0.11\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc97388-763a-4eeb-d82b-f46abfc08c27",
        "id": "Roq9NZQuLvt4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/255.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/255.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Drawing helpers\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose"
      ],
      "metadata": {
        "id": "_-ksqeDhLbgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Train Model"
      ],
      "metadata": {
        "id": "BR8V-mdGMIUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Describe data and split dataset"
      ],
      "metadata": {
        "id": "zynmfQnXMNgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_frame(frame, percent=50):\n",
        "    '''\n",
        "    Rescale a frame to a certain percentage compare to its original frame\n",
        "    '''\n",
        "    width = int(frame.shape[1] * percent/ 100)\n",
        "    height = int(frame.shape[0] * percent/ 100)\n",
        "    dim = (width, height)\n",
        "    return cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "def describe_dataset(dataset_path: str):\n",
        "    '''\n",
        "    Describe dataset\n",
        "    '''\n",
        "\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    print(f\"Headers: {list(data.columns.values)}\")\n",
        "    print(f'Number of rows: {data.shape[0]} \\nNumber of columns: {data.shape[1]}\\n')\n",
        "    print(f\"Labels: \\n{data['label'].value_counts()}\\n\")\n",
        "    print(f\"Missing values: {data.isnull().values.any()}\\n\")\n",
        "\n",
        "    duplicate = data[data.duplicated()]\n",
        "    print(f\"Duplicate Rows : {len(duplicate.sum(axis=1))}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def round_up_metric_results(results) -> list:\n",
        "    '''Round up metrics results such as precision score, recall score, ...'''\n",
        "    return list(map(lambda el: round(el, 3), results))"
      ],
      "metadata": {
        "id": "gXyDGhqGMaGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = describe_dataset(\"./train.csv\")\n",
        "\n",
        "# Categorizing label\n",
        "df.loc[df[\"label\"] == \"C\", \"label\"] = 0\n",
        "df.loc[df[\"label\"] == \"L\", \"label\"] = 1"
      ],
      "metadata": {
        "id": "Pr1a_Wu2MgmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()"
      ],
      "metadata": {
        "id": "BQoLtfTOMlGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./model/input_scaler.pkl\", \"rb\") as f:\n",
        "    sc = pickle.load(f)"
      ],
      "metadata": {
        "id": "LjvA-PEcMnqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Scaling of features\n",
        "x = df.drop(\"label\", axis = 1)\n",
        "x = pd.DataFrame(sc.transform(x))\n",
        "\n",
        "y = df[\"label\"].astype('int')"
      ],
      "metadata": {
        "id": "OJaMeZmaMumP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
        "y_train"
      ],
      "metadata": {
        "id": "j_SgwcZFMw0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Train model using Scikit-learn"
      ],
      "metadata": {
        "id": "Mim1VU8PM6FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "algorithms =[(\"LR\", LogisticRegression()),\n",
        "         (\"SVC\", SVC(probability=True)),\n",
        "         ('KNN',KNeighborsClassifier()),\n",
        "         (\"DTC\", DecisionTreeClassifier()),\n",
        "         (\"SGDC\", CalibratedClassifierCV(SGDClassifier())),\n",
        "         (\"NB\", GaussianNB()),\n",
        "         ('RF', RandomForestClassifier()),]\n",
        "\n",
        "models = {}\n",
        "final_results = []\n",
        "\n",
        "for name, model in algorithms:\n",
        "    trained_model = model.fit(X_train, y_train)\n",
        "    models[name] = trained_model\n",
        "\n",
        "    # Evaluate model\n",
        "    model_results = model.predict(X_test)\n",
        "\n",
        "    p_score = precision_score(y_test, model_results, average=None, labels=[0, 1])\n",
        "    a_score = accuracy_score(y_test, model_results)\n",
        "    r_score = recall_score(y_test, model_results, average=None, labels=[0, 1])\n",
        "    f1_score_result = f1_score(y_test, model_results, average=None, labels=[0, 1])\n",
        "    cm = confusion_matrix(y_test, model_results, labels=[0, 1])\n",
        "    final_results.append(( name,  round_up_metric_results(p_score), a_score, round_up_metric_results(r_score), round_up_metric_results(f1_score_result), cm))\n",
        "\n",
        "# Sort results by F1 score\n",
        "final_results.sort(key=lambda k: sum(k[4]), reverse=True)\n",
        "pd.DataFrame(final_results, columns=[\"Model\", \"Precision Score\", \"Accuracy score\", \"Recall Score\", \"F1 score\", \"Confusion Matrix\"])"
      ],
      "metadata": {
        "id": "AIQtndGcM9EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Dump models pickle"
      ],
      "metadata": {
        "id": "jCxBB0VWNJcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./model/all_sklearn.pkl\", \"wb\") as f:\n",
        "    pickle.dump(models, f)"
      ],
      "metadata": {
        "id": "X711YFkHNMhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./model/input_scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(sc, f)"
      ],
      "metadata": {
        "id": "OJoy9U9_NPgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning"
      ],
      "metadata": {
        "id": "Rnbr7X4oNlNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "_wJlndOHfBvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Train-Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Classification Report\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "wHARcleVfBNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Important Landmarks and Important functions"
      ],
      "metadata": {
        "id": "Ax6G7QBofTC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine important landmarks for plank\n",
        "IMPORTANT_LMS = [\n",
        "    \"NOSE\",\n",
        "    \"LEFT_SHOULDER\",\n",
        "    \"RIGHT_SHOULDER\",\n",
        "    \"RIGHT_ELBOW\",\n",
        "    \"LEFT_ELBOW\",\n",
        "    \"RIGHT_WRIST\",\n",
        "    \"LEFT_WRIST\",\n",
        "    \"LEFT_HIP\",\n",
        "    \"RIGHT_HIP\",\n",
        "]\n",
        "\n",
        "# Generate all columns of the data frame\n",
        "\n",
        "HEADERS = [\"label\"] # Label column\n",
        "\n",
        "for lm in IMPORTANT_LMS:\n",
        "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
      ],
      "metadata": {
        "id": "HxNvwus4fmN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_dataset(dataset_path: str):\n",
        "    '''\n",
        "    Describe dataset\n",
        "    '''\n",
        "\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    print(f\"Headers: {list(data.columns.values)}\")\n",
        "    print(f'Number of rows: {data.shape[0]} \\nNumber of columns: {data.shape[1]}\\n')\n",
        "    print(f\"Labels: \\n{data['label'].value_counts()}\\n\")\n",
        "    print(f\"Missing values: {data.isnull().values.any()}\\n\")\n",
        "\n",
        "    duplicate = data[data.duplicated()]\n",
        "    print(f\"Duplicate Rows : {len(duplicate.sum(axis=1))}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Remove duplicate rows (optional)\n",
        "def remove_duplicate_rows(dataset_path: str):\n",
        "    '''\n",
        "    Remove duplicated data from the dataset then save it to another files\n",
        "    '''\n",
        "\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df.drop_duplicates(keep=\"first\", inplace=True)\n",
        "    df.to_csv(f\"cleaned_train.csv\", sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "def round_up_metric_results(results) -> list:\n",
        "    '''Round up metrics results such as precision score, recall score, ...'''\n",
        "    return list(map(lambda el: round(el, 3), results))"
      ],
      "metadata": {
        "id": "aVvy1dlXfo_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Describe Dataset & Split Data"
      ],
      "metadata": {
        "id": "93RvVkTAfxpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = describe_dataset(\"./train.csv\")\n",
        "\n",
        "# Categorizing label\n",
        "df.loc[df[\"label\"] == \"C\", \"label\"] = 0\n",
        "df.loc[df[\"label\"] == \"L\", \"label\"] = 1"
      ],
      "metadata": {
        "id": "Hw4jZoPtf3B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./model/input_scaler.pkl\", \"rb\") as f:\n",
        "    sc = pickle.load(f)"
      ],
      "metadata": {
        "id": "N5NOvhQ2f4wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Scaling of features\n",
        "x = df.drop(\"label\", axis = 1)\n",
        "x = pd.DataFrame(sc.transform(x))\n",
        "\n",
        "y = df[\"label\"]\n",
        "\n",
        "# # Converting prediction to categorical\n",
        "y_cat = to_categorical(y)"
      ],
      "metadata": {
        "id": "1RC452NXf6Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x.values, y_cat, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "iYU2VOUQgWhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build Model"
      ],
      "metadata": {
        "id": "COsKaK9ggY3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Setup"
      ],
      "metadata": {
        "id": "YHnGQKV1gc2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Final Results\n",
        "final_models = {}"
      ],
      "metadata": {
        "id": "Tzh0N-qageYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_model(model):\n",
        "    '''\n",
        "    Describe Model architecture\n",
        "    '''\n",
        "    print(f\"Describe models architecture\")\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        number_of_units = layer.units if hasattr(layer, 'units') else 0\n",
        "\n",
        "        if hasattr(layer, \"activation\"):\n",
        "            print(f\"Layer-{i + 1}: {number_of_units} units, func: \", layer.activation)\n",
        "        else:\n",
        "            print(f\"Layer-{i + 1}: {number_of_units} units, func: None\")\n",
        "\n",
        "\n",
        "def get_best_model(tuner):\n",
        "    '''\n",
        "    Describe and return the best model found from keras tuner\n",
        "    '''\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    describe_model(best_model)\n",
        "\n",
        "    print(\"\\nOther params:\")\n",
        "    ignore_params = [\"tuner\", \"activation\", \"layer\", \"epoch\"]\n",
        "    for param, value in best_hps.values.items():\n",
        "        if not any(word in param for word in ignore_params):\n",
        "            print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "dcJlqj0MgbFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Model with 3 Layers"
      ],
      "metadata": {
        "id": "LbkhJ3uCgldu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_3l_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(36, input_dim = 36, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZDPsqqemgi7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_3l = kt.Hyperband(\n",
        "    model_3l_builder,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo',\n",
        ")\n",
        "tuner_3l.search(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "8pM4v_4ogo8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3l = get_best_model(tuner_3l)\n",
        "model_3l.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "e0gDSzg_gxAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_models[\"3_layers\"] = model_3l"
      ],
      "metadata": {
        "id": "6i--FPFLgxeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Model with 5 layers"
      ],
      "metadata": {
        "id": "aIniD1NEgreH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_5l_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(36, input_dim = 36, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_3 = hp.Int('layer_3', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_3, activation=hp_activation))\n",
        "    model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XqTkpASag2UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_5l = kt.Hyperband(\n",
        "    model_5l_builder,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo_2'\n",
        ")\n",
        "tuner_5l.search(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[stop_early, TensorBoard(\"./keras_tuner_dir/logs\")])"
      ],
      "metadata": {
        "id": "NStCpeUcg40R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5l = get_best_model(tuner_5l)\n",
        "model_5l.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "sg9_ms13g61D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_models[\"5_layers\"] = model_5l"
      ],
      "metadata": {
        "id": "kw-uK745g885"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 Model with 7 layers include Dropout"
      ],
      "metadata": {
        "id": "LXK_3iMJhCi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_7lD_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(36, input_dim = 36, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_3 = hp.Int('layer_3', min_value=32, max_value=512, step=32)\n",
        "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    hp_dropout_2 = hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dropout(rate=hp_dropout_1))\n",
        "    model.add(Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(Dropout(rate=hp_dropout_2))\n",
        "    model.add(Dense(units=hp_layer_3, activation=hp_activation))\n",
        "    model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GvBC1U0KhGXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_7lD = kt.Hyperband(\n",
        "    model_7lD_builder,\n",
        "    objective='accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo_3'\n",
        ")\n",
        "tuner_7lD.search(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[stop_early, TensorBoard(\"./keras_tuner_dir/logs\")])"
      ],
      "metadata": {
        "id": "FkObv5YDhIua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_7lD = get_best_model(tuner_7lD)\n",
        "model_7lD.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "DkO7kWK0hLYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_models[\"7_layers_with_dropout\"] = model_7lD"
      ],
      "metadata": {
        "id": "Fpx0Puu-hOCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.5 Model with 7 layers"
      ],
      "metadata": {
        "id": "nMwNEEPchR-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_7l_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(36, input_dim = 36, activation = \"relu\"))\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_3 = hp.Int('layer_3', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_4 = hp.Int('layer_4', min_value=32, max_value=512, step=32)\n",
        "    hp_layer_5 = hp.Int('layer_5', min_value=32, max_value=512, step=32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.add(Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_3, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_4, activation=hp_activation))\n",
        "    model.add(Dense(units=hp_layer_5, activation=hp_activation))\n",
        "    model.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3Txk2dOjhU_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_7l = kt.Hyperband(\n",
        "    model_7l_builder,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='keras_tuner_demo_6'\n",
        ")\n",
        "tuner_7l.search(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[stop_early, TensorBoard(\"./keras_tuner_dir/logs\")])"
      ],
      "metadata": {
        "id": "BBjx4JezhXGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_7l = get_best_model(tuner_7l)\n",
        "model_7l.fit(x_train, y_train, epochs=100, batch_size=10, validation_data=(x_test, y_test), callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "_sEE1zf3hZdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_models[\"7_layers\"] = model_7l"
      ],
      "metadata": {
        "id": "sQ2GyjEJhbC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.6 Describe final models"
      ],
      "metadata": {
        "id": "Ku4XF9uvhdA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in final_models.items():\n",
        "    print(f\"{name}: \", end=\"\")\n",
        "    describe_model(model)\n",
        "    print()"
      ],
      "metadata": {
        "id": "D0cS03C2hgGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Evaluation"
      ],
      "metadata": {
        "id": "Xgk5VEUIhjyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 Train set evaluation"
      ],
      "metadata": {
        "id": "QdgHC_xMhmvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_results = []\n",
        "\n",
        "for name, model in final_models.items():\n",
        "    # Evaluate model\n",
        "    predict_x = model.predict(x_test, verbose=False)\n",
        "    y_pred_class = np.argmax(predict_x, axis=1)\n",
        "    y_test_class = np.argmax(y_test, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test_class, y_pred_class, labels=[0, 1])\n",
        "    (p_score, r_score, f_score, _) = precision_recall_fscore_support(y_test_class, y_pred_class, labels=[0, 1])\n",
        "\n",
        "    train_set_results.append(( name, round_up_metric_results(p_score), round_up_metric_results(r_score), round_up_metric_results(f_score), cm ))\n",
        "\n",
        "train_set_results.sort(key=lambda k: sum(k[3]), reverse=True)\n",
        "pd.DataFrame(train_set_results, columns=[\"Model\", \"Precision Score\", \"Recall Score\", \"F1 score\", \"Confusion Matrix\"])"
      ],
      "metadata": {
        "id": "I-A_ab5ChqN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Test Set Evaluation"
      ],
      "metadata": {
        "id": "RMS8-DtdhwN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "test_df = describe_dataset(\"./test.csv\")\n",
        "\n",
        "# Categorizing label\n",
        "test_df.loc[test_df[\"label\"] == \"C\", \"label\"] = 0\n",
        "test_df.loc[test_df[\"label\"] == \"L\", \"label\"] = 1"
      ],
      "metadata": {
        "id": "_umzdHOuhsMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Scaling of features\n",
        "test_x = test_df.drop(\"label\", axis = 1)\n",
        "test_x = pd.DataFrame(sc.transform(test_x))\n",
        "\n",
        "test_y = test_df[\"label\"]\n",
        "\n",
        "# # Converting prediction to categorical\n",
        "test_y_cat = to_categorical(test_y)"
      ],
      "metadata": {
        "id": "6J31-eC9h1UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_results = []\n",
        "\n",
        "for name, model in final_models.items():\n",
        "    # Evaluate model\n",
        "    predict_x = model.predict(test_x, verbose=False)\n",
        "    y_pred_class = np.argmax(predict_x, axis=1)\n",
        "    y_test_class = np.argmax(test_y_cat, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test_class, y_pred_class, labels=[0, 1])\n",
        "    (p_score, r_score, f_score, _) = precision_recall_fscore_support(y_test_class, y_pred_class, labels=[0, 1])\n",
        "\n",
        "    test_set_results.append(( name, round_up_metric_results(p_score), round_up_metric_results(r_score), round_up_metric_results(f_score), cm ))\n",
        "\n",
        "test_set_results.sort(key=lambda k: k[1] + k[2] + k[3], reverse=True)\n",
        "pd.DataFrame(test_set_results, columns=[\"Model\", \"Precision Score\", \"Recall Score\", \"F1 score\", \"Confusion Matrix\"])"
      ],
      "metadata": {
        "id": "N9jZiWuYh26m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Dumped Model"
      ],
      "metadata": {
        "id": "UX8aCChGh7y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump the best model to a pickle file\n",
        "with open(\"./model/bicep_dp.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_models[\"7_layers\"], f)"
      ],
      "metadata": {
        "id": "SUTkcATZh5WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump final results\n",
        "with open(\"./model/all_models.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_models, f)"
      ],
      "metadata": {
        "id": "YMDgS6t4h_zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection"
      ],
      "metadata": {
        "id": "dl2goZ0fc225"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "7hq-AVIjdJcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Run debug below if mediapipe module didn't install properly"
      ],
      "metadata": {
        "id": "Qf0SzHseqAQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvNS8apyqAQt"
      },
      "outputs": [],
      "source": [
        "# DEBUG\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv_jupyter_ui\n",
        "!pip install -q ipycanvas==0.11\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "jTy5ZjKkqAQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import opencv_jupyter_ui as jcv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Drawing helpers\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "jcv2.setKeys(['q','c','l'])"
      ],
      "metadata": {
        "id": "FHLnlHc-p2gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set up important functions"
      ],
      "metadata": {
        "id": "KPlO-b1kqIkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine important landmarks for plank\n",
        "IMPORTANT_LMS = [\n",
        "    \"NOSE\",\n",
        "    \"LEFT_SHOULDER\",\n",
        "    \"RIGHT_SHOULDER\",\n",
        "    \"RIGHT_ELBOW\",\n",
        "    \"LEFT_ELBOW\",\n",
        "    \"RIGHT_WRIST\",\n",
        "    \"LEFT_WRIST\",\n",
        "    \"LEFT_HIP\",\n",
        "    \"RIGHT_HIP\",\n",
        "]\n",
        "\n",
        "# Generate all columns of the data frame\n",
        "\n",
        "HEADERS = [\"label\"] # Label column\n",
        "\n",
        "for lm in IMPORTANT_LMS:\n",
        "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
      ],
      "metadata": {
        "id": "1PFS8FI8qKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_frame(frame, percent=50):\n",
        "    '''\n",
        "    Rescale a frame from OpenCV to a certain percentage compare to its original frame\n",
        "    '''\n",
        "    width = int(frame.shape[1] * percent/ 100)\n",
        "    height = int(frame.shape[0] * percent/ 100)\n",
        "    dim = (width, height)\n",
        "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "def save_frame_as_image(frame, message: str = None):\n",
        "    '''\n",
        "    Save a frame as image to display the error\n",
        "    '''\n",
        "    now = datetime.datetime.now()\n",
        "\n",
        "    if message:\n",
        "        cv2.putText(frame, message, (50, 150), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "    print(\"Saving ...\")\n",
        "    cv2.imwrite(f\"../data/logs/bicep_{now}.jpg\", frame)\n",
        "\n",
        "\n",
        "def calculate_angle(point1: list, point2: list, point3: list) -> float:\n",
        "    '''\n",
        "    Calculate the angle between 3 points\n",
        "    Unit of the angle will be in Degree\n",
        "    '''\n",
        "    point1 = np.array(point1)\n",
        "    point2 = np.array(point2)\n",
        "    point3 = np.array(point3)\n",
        "\n",
        "    # Calculate algo\n",
        "    angleInRad = np.arctan2(point3[1] - point2[1], point3[0] - point2[0]) - np.arctan2(point1[1] - point2[1], point1[0] - point2[0])\n",
        "    angleInDeg = np.abs(angleInRad * 180.0 / np.pi)\n",
        "\n",
        "    angleInDeg = angleInDeg if angleInDeg <= 180 else 360 - angleInDeg\n",
        "    return angleInDeg\n",
        "\n",
        "\n",
        "def extract_important_keypoints(results, important_landmarks: list) -> list:\n",
        "    '''\n",
        "    Extract important keypoints from mediapipe pose detection\n",
        "    '''\n",
        "    landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "    data = []\n",
        "    for lm in important_landmarks:\n",
        "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
        "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
        "\n",
        "    return np.array(data).flatten().tolist()"
      ],
      "metadata": {
        "id": "G-Aiu9ptqM_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. OOP method for Analyze Pose of each arm"
      ],
      "metadata": {
        "id": "oV1VKt4mqQEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BicepPoseAnalysis:\n",
        "    def __init__(self, side: str, stage_down_threshold: float, stage_up_threshold: float, peak_contraction_threshold: float, loose_upper_arm_angle_threshold: float, visibility_threshold: float):\n",
        "        # Initialize thresholds\n",
        "        self.stage_down_threshold = stage_down_threshold\n",
        "        self.stage_up_threshold = stage_up_threshold\n",
        "        self.peak_contraction_threshold = peak_contraction_threshold\n",
        "        self.loose_upper_arm_angle_threshold = loose_upper_arm_angle_threshold\n",
        "        self.visibility_threshold = visibility_threshold\n",
        "\n",
        "        self.side = side\n",
        "        self.counter = 0\n",
        "        self.stage = \"down\"\n",
        "        self.is_visible = True\n",
        "        self.detected_errors = {\n",
        "            \"LOOSE_UPPER_ARM\": 0,\n",
        "            \"PEAK_CONTRACTION\": 0,\n",
        "        }\n",
        "\n",
        "        # Params for loose upper arm error detection\n",
        "        self.loose_upper_arm = False\n",
        "\n",
        "        # Params for peak contraction error detection\n",
        "        self.peak_contraction_angle = 1000\n",
        "        self.peak_contraction_frame = None\n",
        "\n",
        "    def get_joints(self, landmarks) -> bool:\n",
        "        '''\n",
        "        Check for joints' visibility then get joints coordinate\n",
        "        '''\n",
        "        side = self.side.upper()\n",
        "\n",
        "        # Check visibility\n",
        "        joints_visibility = [ landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].visibility, landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].visibility, landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].visibility ]\n",
        "\n",
        "        is_visible = all([ vis > self.visibility_threshold for vis in joints_visibility ])\n",
        "        self.is_visible = is_visible\n",
        "\n",
        "        if not is_visible:\n",
        "            return self.is_visible\n",
        "\n",
        "        # Get joints' coordinates\n",
        "        self.shoulder = [ landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].y ]\n",
        "        self.elbow = [ landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].y ]\n",
        "        self.wrist = [ landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].y ]\n",
        "\n",
        "        return self.is_visible\n",
        "\n",
        "    def analyze_pose(self, landmarks, frame):\n",
        "        '''\n",
        "        - Bicep Counter\n",
        "        - Errors Detection\n",
        "        '''\n",
        "        self.get_joints(landmarks)\n",
        "\n",
        "        # Cancel calculation if visibility is poor\n",
        "        if not self.is_visible:\n",
        "            return (None, None)\n",
        "\n",
        "        # * Calculate curl angle for counter\n",
        "        bicep_curl_angle = int(calculate_angle(self.shoulder, self.elbow, self.wrist))\n",
        "        if bicep_curl_angle > self.stage_down_threshold:\n",
        "            self.stage = \"down\"\n",
        "        elif bicep_curl_angle < self.stage_up_threshold and self.stage == \"down\":\n",
        "            self.stage = \"up\"\n",
        "            self.counter += 1\n",
        "\n",
        "        # * Calculate the angle between the upper arm (shoulder & joint) and the Y axis\n",
        "        shoulder_projection = [ self.shoulder[0], 1 ] # Represent the projection of the shoulder to the X axis\n",
        "        ground_upper_arm_angle = int(calculate_angle(self.elbow, self.shoulder, shoulder_projection))\n",
        "\n",
        "        # * Evaluation for LOOSE UPPER ARM error\n",
        "        if ground_upper_arm_angle > self.loose_upper_arm_angle_threshold:\n",
        "            # Limit the saved frame\n",
        "            if not self.loose_upper_arm:\n",
        "                self.loose_upper_arm = True\n",
        "                # save_frame_as_image(frame, f\"Loose upper arm: {ground_upper_arm_angle}\")\n",
        "                self.detected_errors[\"LOOSE_UPPER_ARM\"] += 1\n",
        "        else:\n",
        "            self.loose_upper_arm = False\n",
        "\n",
        "        # * Evaluate PEAK CONTRACTION error\n",
        "        if self.stage == \"up\" and bicep_curl_angle < self.peak_contraction_angle:\n",
        "            # Save peaked contraction every rep\n",
        "            self.peak_contraction_angle = bicep_curl_angle\n",
        "            self.peak_contraction_frame = frame\n",
        "\n",
        "        elif self.stage == \"down\":\n",
        "            # * Evaluate if the peak is higher than the threshold if True, marked as an error then saved that frame\n",
        "            if self.peak_contraction_angle != 1000 and self.peak_contraction_angle >= self.peak_contraction_threshold:\n",
        "                # save_frame_as_image(self.peak_contraction_frame, f\"{self.side} - Peak Contraction: {self.peak_contraction_angle}\")\n",
        "                self.detected_errors[\"PEAK_CONTRACTION\"] += 1\n",
        "\n",
        "            # Reset params\n",
        "            self.peak_contraction_angle = 1000\n",
        "            self.peak_contraction_frame = None\n",
        "\n",
        "        return (bicep_curl_angle, ground_upper_arm_angle)"
      ],
      "metadata": {
        "id": "VUfUOBjjqUGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Bicep Detection"
      ],
      "metadata": {
        "id": "p5Ww0U9cqbhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Change the File Path"
      ],
      "metadata": {
        "id": "DF0CnDYXqiWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_DEMO_PATH = \"../../demo/bc_demo.mp4\""
      ],
      "metadata": {
        "id": "NdwULoMgqdS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input scaler\n",
        "with open(\"./model/input_scaler.pkl\", \"rb\") as f:\n",
        "    input_scaler = pickle.load(f)"
      ],
      "metadata": {
        "id": "GmDN9fNuqrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. Detection with SKLearn model"
      ],
      "metadata": {
        "id": "Ho-49xlPqtJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "with open(\"./model/KNN_model.pkl\", \"rb\") as f:\n",
        "    sklearn_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "RYRLaO1TqwGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(VIDEO_DEMO_PATH)\n",
        "\n",
        "VISIBILITY_THRESHOLD = 0.65\n",
        "\n",
        "# Params for counter\n",
        "STAGE_UP_THRESHOLD = 90\n",
        "STAGE_DOWN_THRESHOLD = 120\n",
        "\n",
        "# Params to catch FULL RANGE OF MOTION error\n",
        "PEAK_CONTRACTION_THRESHOLD = 60\n",
        "\n",
        "# LOOSE UPPER ARM error detection\n",
        "LOOSE_UPPER_ARM = False\n",
        "LOOSE_UPPER_ARM_ANGLE_THRESHOLD = 40\n",
        "\n",
        "# STANDING POSTURE error detection\n",
        "POSTURE_ERROR_THRESHOLD = 0.7\n",
        "posture = \"C\"\n",
        "\n",
        "# Init analysis class\n",
        "left_arm_analysis = BicepPoseAnalysis(side=\"left\", stage_down_threshold=STAGE_DOWN_THRESHOLD, stage_up_threshold=STAGE_UP_THRESHOLD, peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD, loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD, visibility_threshold=VISIBILITY_THRESHOLD)\n",
        "\n",
        "right_arm_analysis = BicepPoseAnalysis(side=\"right\", stage_down_threshold=STAGE_DOWN_THRESHOLD, stage_up_threshold=STAGE_UP_THRESHOLD, peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD, loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD, visibility_threshold=VISIBILITY_THRESHOLD)\n",
        "\n",
        "with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Reduce size of a frame\n",
        "        image = rescale_frame(image, 50)\n",
        "        # image = cv2.flip(image, 1)\n",
        "\n",
        "        video_dimensions = [image.shape[1], image.shape[0]]\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        results = pose.process(image)\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "            print(\"No human found\")\n",
        "            continue\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image.flags.writeable = True\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
        "\n",
        "        # Make detection\n",
        "        try:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            (left_bicep_curl_angle, left_ground_upper_arm_angle) = left_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
        "            (right_bicep_curl_angle, right_ground_upper_arm_angle) = right_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
        "\n",
        "            # Extract keypoints from frame for the input\n",
        "            row = extract_important_keypoints(results, IMPORTANT_LMS)\n",
        "            X = pd.DataFrame([row], columns=HEADERS[1:])\n",
        "            X = pd.DataFrame(input_scaler.transform(X))\n",
        "\n",
        "\n",
        "            # Make prediction and its probability\n",
        "            predicted_class = sklearn_model.predict(X)[0]\n",
        "            prediction_probabilities = sklearn_model.predict_proba(X)[0]\n",
        "            class_prediction_probability = round(prediction_probabilities[np.argmax(prediction_probabilities)], 2)\n",
        "\n",
        "            if class_prediction_probability >= POSTURE_ERROR_THRESHOLD:\n",
        "                posture = predicted_class\n",
        "\n",
        "            # Visualization\n",
        "            # Status box\n",
        "            cv2.rectangle(image, (0, 0), (500, 40), (245, 117, 16), -1)\n",
        "\n",
        "            # Display probability\n",
        "            cv2.putText(image, \"RIGHT\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(right_arm_analysis.counter) if right_arm_analysis.is_visible else \"UNK\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Display Left Counter\n",
        "            cv2.putText(image, \"LEFT\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(left_arm_analysis.counter) if left_arm_analysis.is_visible else \"UNK\", (100, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # * Display error\n",
        "            # Right arm error\n",
        "            cv2.putText(image, \"R_PC\", (165, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(right_arm_analysis.detected_errors[\"PEAK_CONTRACTION\"]), (160, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "            cv2.putText(image, \"R_LUA\", (225, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(right_arm_analysis.detected_errors[\"LOOSE_UPPER_ARM\"]), (220, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Left arm error\n",
        "            cv2.putText(image, \"L_PC\", (300, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(left_arm_analysis.detected_errors[\"PEAK_CONTRACTION\"]), (295, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "            cv2.putText(image, \"L_LUA\", (380, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(left_arm_analysis.detected_errors[\"LOOSE_UPPER_ARM\"]), (375, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Lean back error\n",
        "            cv2.putText(image, \"LB\", (460, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(f\"{posture}, {predicted_class}, {class_prediction_probability}\"), (440, 30), cv2.FONT_HERSHEY_COMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "            # * Visualize angles\n",
        "            # Visualize LEFT arm calculated angles\n",
        "            if left_arm_analysis.is_visible:\n",
        "                cv2.putText(image, str(left_bicep_curl_angle), tuple(np.multiply(left_arm_analysis.elbow, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "                cv2.putText(image, str(left_ground_upper_arm_angle), tuple(np.multiply(left_arm_analysis.shoulder, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "            # Visualize RIGHT arm calculated angles\n",
        "            if right_arm_analysis.is_visible:\n",
        "                cv2.putText(image, str(right_bicep_curl_angle), tuple(np.multiply(right_arm_analysis.elbow, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
        "                cv2.putText(image, str(right_ground_upper_arm_angle), tuple(np.multiply(right_arm_analysis.shoulder, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        jcv2.imshow(\"CV2\", image)\n",
        "\n",
        "        # if left_arm_analysis.loose_upper_arm:\n",
        "        #     save_frame_as_image(image, \"\")\n",
        "\n",
        "        # Press Q to close cv2 window\n",
        "        if jcv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    jcv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "_1pP4GIKqx63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Detection with Deep Learning model"
      ],
      "metadata": {
        "id": "NNe7leC3q436"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "with open(\"./model/bicep_dp.pkl\", \"rb\") as f:\n",
        "    DL_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "QXJo0ICVq6SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(VIDEO_DEMO_PATH)\n",
        "\n",
        "VISIBILITY_THRESHOLD = 0.65\n",
        "\n",
        "# Params for counter\n",
        "STAGE_UP_THRESHOLD = 90\n",
        "STAGE_DOWN_THRESHOLD = 120\n",
        "\n",
        "# Params to catch FULL RANGE OF MOTION error\n",
        "PEAK_CONTRACTION_THRESHOLD = 60\n",
        "\n",
        "# LOOSE UPPER ARM error detection\n",
        "LOOSE_UPPER_ARM = False\n",
        "LOOSE_UPPER_ARM_ANGLE_THRESHOLD = 40\n",
        "\n",
        "# STANDING POSTURE error detection\n",
        "POSTURE_ERROR_THRESHOLD = 0.95\n",
        "posture = 0\n",
        "\n",
        "# Init analysis class\n",
        "left_arm_analysis = BicepPoseAnalysis(side=\"left\", stage_down_threshold=STAGE_DOWN_THRESHOLD, stage_up_threshold=STAGE_UP_THRESHOLD, peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD, loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD, visibility_threshold=VISIBILITY_THRESHOLD)\n",
        "\n",
        "right_arm_analysis = BicepPoseAnalysis(side=\"right\", stage_down_threshold=STAGE_DOWN_THRESHOLD, stage_up_threshold=STAGE_UP_THRESHOLD, peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD, loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD, visibility_threshold=VISIBILITY_THRESHOLD)\n",
        "\n",
        "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Reduce size of a frame\n",
        "        image = rescale_frame(image, 40)\n",
        "        # image = cv2.flip(image, 1)\n",
        "\n",
        "        video_dimensions = [image.shape[1], image.shape[0]]\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        results = pose.process(image)\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "            print(\"No human found\")\n",
        "            continue\n",
        "\n",
        "        # Recolor image from BGR to RGB for mediapipe\n",
        "        image.flags.writeable = True\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Draw landmarks and connections\n",
        "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
        "\n",
        "        # Make detection\n",
        "        try:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            (left_bicep_curl_angle, left_ground_upper_arm_angle) = left_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
        "            (right_bicep_curl_angle, right_ground_upper_arm_angle) = right_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
        "\n",
        "            # Extract keypoints from frame for the input\n",
        "            row = extract_important_keypoints(results, IMPORTANT_LMS)\n",
        "            X = pd.DataFrame([row, ], columns=HEADERS[1:])\n",
        "            X = pd.DataFrame(input_scaler.transform(X))\n",
        "\n",
        "            # Make prediction and its probability\n",
        "            prediction = DL_model.predict(X)\n",
        "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "            prediction_probability = round(max(prediction.tolist()[0]), 2)\n",
        "\n",
        "            if prediction_probability >= POSTURE_ERROR_THRESHOLD:\n",
        "                posture = predicted_class\n",
        "\n",
        "            # Visualization\n",
        "            # Status box\n",
        "            cv2.rectangle(image, (0, 0), (500, 40), (245, 117, 16), -1)\n",
        "\n",
        "            # Display probability\n",
        "            cv2.putText(image, \"RIGHT\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(right_arm_analysis.counter) if right_arm_analysis.is_visible else \"UNK\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Display Left Counter\n",
        "            cv2.putText(image, \"LEFT\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(left_arm_analysis.counter) if left_arm_analysis.is_visible else \"UNK\", (100, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # * Display error\n",
        "            # Right arm error\n",
        "            cv2.putText(image, \"R_PC\", (165, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(right_arm_analysis.detected_errors[\"PEAK_CONTRACTION\"]), (160, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "            cv2.putText(image, \"R_LUA\", (225, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(right_arm_analysis.detected_errors[\"LOOSE_UPPER_ARM\"]), (220, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Left arm error\n",
        "            cv2.putText(image, \"L_PC\", (300, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(left_arm_analysis.detected_errors[\"PEAK_CONTRACTION\"]), (295, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "            cv2.putText(image, \"L_LUA\", (380, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(left_arm_analysis.detected_errors[\"LOOSE_UPPER_ARM\"]), (375, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Lean back error\n",
        "            cv2.putText(image, \"LB\", (460, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "            cv2.putText(image, str(\"C\" if posture == 0 else \"L\") + f\" ,{predicted_class}, {prediction_probability}\", (440, 30), cv2.FONT_HERSHEY_COMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "            # * Visualize angles\n",
        "            # Visualize LEFT arm calculated angles\n",
        "            if left_arm_analysis.is_visible:\n",
        "                cv2.putText(image, str(left_bicep_curl_angle), tuple(np.multiply(left_arm_analysis.elbow, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "                cv2.putText(image, str(left_ground_upper_arm_angle), tuple(np.multiply(left_arm_analysis.shoulder, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "            # Visualize RIGHT arm calculated angles\n",
        "            if right_arm_analysis.is_visible:\n",
        "                cv2.putText(image, str(right_bicep_curl_angle), tuple(np.multiply(right_arm_analysis.elbow, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
        "                cv2.putText(image, str(right_ground_upper_arm_angle), tuple(np.multiply(right_arm_analysis.shoulder, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        cv2.imshow(\"CV2\", image)\n",
        "\n",
        "        # Press Q to close cv2 window\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "4bCsIOLcq8T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OMSNdLDOLCw"
      },
      "source": [
        "# **IMG VID CAPTURE** (<font color='red'>Not Important!!!</font>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owaUOf4n7g_y"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKpzl0e17g_y"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqoxlqqvOKKo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN7JQ2x48whB"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oUJtTwf8zPp"
      },
      "outputs": [],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXG94EyAO94e"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "A2-4JFOqPSDA",
        "YcTU4-PFPAWt",
        "zElAl81XSEPM",
        "I7pdjBhBSzXJ",
        "kW5aAn9JE05s",
        "Qm-VHj3OGMTA",
        "aIniD1NEgreH",
        "LXK_3iMJhCi5",
        "nMwNEEPchR-L",
        "RMS8-DtdhwN5",
        "7OMSNdLDOLCw"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvnykB7Y/I2ZsiLeuzDY51",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}